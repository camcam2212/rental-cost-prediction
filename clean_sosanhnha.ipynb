{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Merge all files of sonhanha.com"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import numpy as np\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score\n",
    "from xgboost import XGBRegressor\n",
    "from xgboost import XGBClassifier\n",
    "import pandas as pd\n",
    "\n",
    "import selenium\n",
    "from selenium import webdriver\n",
    "from time import sleep\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.common.by import By\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "import os\n",
    "\n",
    "import re\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 15180 entries, 0 to 15179\n",
      "Data columns (total 16 columns):\n",
      " #   Column              Non-Null Count  Dtype  \n",
      "---  ------              --------------  -----  \n",
      " 0   Title               14805 non-null  object \n",
      " 1   Price               14813 non-null  object \n",
      " 2   Area                14813 non-null  object \n",
      " 3   address             2813 non-null   object \n",
      " 4   type                15180 non-null  object \n",
      " 5   area_value          2813 non-null   float64\n",
      " 6   price_value_trieu   2807 non-null   float64\n",
      " 7   price_per_m2_final  2807 non-null   float64\n",
      " 8   House_number        627 non-null    object \n",
      " 9   Street              2523 non-null   object \n",
      " 10  Ward                2649 non-null   object \n",
      " 11  District            2760 non-null   object \n",
      " 12  City                2771 non-null   object \n",
      " 13  progress            2813 non-null   object \n",
      " 14  cleaned_street      2813 non-null   object \n",
      " 15  Location            12000 non-null  object \n",
      "dtypes: float64(3), object(13)\n",
      "memory usage: 1.9+ MB\n"
     ]
    }
   ],
   "source": [
    "folder_path = \"D:\\OneDrive\\KiotViet\\Python_for_work\\KFinance\\CSV_crawl_data\"\n",
    "file_list = os.listdir(folder_path)\n",
    "\n",
    "merged_df_new = pd.DataFrame()\n",
    "\n",
    "# Loop qua từng file CSV và merge vào DataFrame\n",
    "for file in file_list:\n",
    "    if \"cho_thue_cua_hang_kiot\" in file or \"sosanhnha_\" in file:\n",
    "        file_path = os.path.join(folder_path, file)\n",
    "        df = pd.read_csv(file_path)\n",
    "        merged_df_new = pd.concat([merged_df_new, df], ignore_index=True)\n",
    "\n",
    "# print(len(merged_df))\n",
    "merged_df_new.drop_duplicates()\n",
    "merged_df_new.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df_new = merged_df_new.dropna(axis=0)\n",
    "merged_df_new.to_csv(r\"D:\\OneDrive\\KiotViet\\Python_for_work\\KFinance\\CSV_crawl_data\\all_bds_sosanhnha.csv\", index=False, mode = 'w', encoding=\"utf-8-sig\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Clean data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The total rows of dataframe is 0\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(r\"D:\\OneDrive\\KiotViet\\Python_for_work\\KFinance\\CSV_crawl_data\\all_bds_sosanhnha.csv\")\n",
    "df.head()\n",
    "print(f\"The total rows of dataframe is {len(df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The total rows of dataframe is 0\n"
     ]
    }
   ],
   "source": [
    "df = df[~df['Area'].str.contains('Đang cập nhật')]\n",
    "df = df[~df['Price'].str.contains('Liên hệ')]\n",
    "print(f\"The total rows of dataframe is {len(df)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.0 Xử lý diện tích"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# area_pattern = re.compile(r'(\\d+) m²')\n",
    "# df['area_value'] = df['Area'].apply(lambda x: int(area_pattern.search(x).group(1)) if area_pattern.search(x) else None)\n",
    "\n",
    "df['area_value'] = df['Area'].apply(lambda x: float(x.replace('Diện tích: ', '').replace(' m²', ''))) #.replace('.', '')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Price</th>\n",
       "      <th>Area</th>\n",
       "      <th>address</th>\n",
       "      <th>type</th>\n",
       "      <th>area_value</th>\n",
       "      <th>price_value_trieu</th>\n",
       "      <th>price_per_m2_final</th>\n",
       "      <th>House_number</th>\n",
       "      <th>Street</th>\n",
       "      <th>Ward</th>\n",
       "      <th>District</th>\n",
       "      <th>City</th>\n",
       "      <th>progress</th>\n",
       "      <th>cleaned_street</th>\n",
       "      <th>Location</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [Title, Price, Area, address, type, area_value, price_value_trieu, price_per_m2_final, House_number, Street, Ward, District, City, progress, cleaned_street, Location]\n",
       "Index: []"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.1 Xử lý price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tạo cột 'price_per_m2'\n",
    "\n",
    "#1.Nếu Price ghi X triệu/tháng và Area ghi Y m2 thì => Cột 'price_per_m2' thành x / Y\n",
    "def convert_price_string_to_float(price_string):\n",
    "    return float(price_string.replace(',', '.'))\n",
    "\n",
    "price_pattern = re.compile(r'([\\d,]+) (triệu/tháng|triệu)')\n",
    "\n",
    "df['price_value_trieu'] = df['Price'].apply(lambda x: convert_price_string_to_float(price_pattern.search(x).group(1)) if price_pattern.search(x) else None)\n",
    "df['price_per_m2_trieu'] = 1000000 * df['price_value_trieu'] / df['area_value']\n",
    "\n",
    "#2.Nếu Price ghi X tỷ và Area ghi Y m2 thì => Cột 'price_per_m2' thành x / Y\n",
    "\n",
    "price_pattern_bil = re.compile(r'([\\d,]+) tỷ')\n",
    "\n",
    "df['price_value_ty'] = df['Price'].apply(lambda x: convert_price_string_to_float(price_pattern_bil.search(x).group(1)) if price_pattern_bil.search(x) else None)\n",
    "df['price_per_m2_ty'] = 1000000000 * df['price_value_ty'] / df['area_value']\n",
    "\n",
    "df['price_value_trieu'] = df['price_value_trieu'].fillna(df['price_value_ty']*1000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#4. Từ các cột \"price_per_m2\", \"price_per_m2_trieu\", \"price_per_m2_ty\" ta tìm được price_per_m2_final\n",
    "\n",
    "def calculate_price_per_m2_final(row):\n",
    "    if not pd.isna(row['price_per_m2_trieu']):\n",
    "        return row['price_per_m2_trieu']\n",
    "    elif not pd.isna(row['price_per_m2_ty']):\n",
    "        return row['price_per_m2_ty']\n",
    "    else:\n",
    "        return np.nan\n",
    "\n",
    "# Áp dụng hàm tùy chỉnh để tạo cột mới\n",
    "df['price_per_m2_final'] = df.apply(calculate_price_per_m2_final, axis=1)\n",
    "df = df.drop(columns=['price_per_m2_trieu','price_value_ty','price_per_m2_ty'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.2. Xử lý địa chỉ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Price</th>\n",
       "      <th>Area</th>\n",
       "      <th>address</th>\n",
       "      <th>type</th>\n",
       "      <th>area_value</th>\n",
       "      <th>price_value_trieu</th>\n",
       "      <th>price_per_m2_final</th>\n",
       "      <th>House_number</th>\n",
       "      <th>Street</th>\n",
       "      <th>Ward</th>\n",
       "      <th>District</th>\n",
       "      <th>City</th>\n",
       "      <th>progress</th>\n",
       "      <th>cleaned_street</th>\n",
       "      <th>address</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [Title, Price, Area, address, type, area_value, price_value_trieu, price_per_m2_final, House_number, Street, Ward, District, City, progress, cleaned_street, address]\n",
       "Index: []"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.rename(columns={'Location': 'address'}, inplace=True)\n",
    "df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_api_keys = [\"1EotnhALO_Xf7d5mlOVefECdfpbgPG_Dik0TeOcQauQ\",\n",
    "\"92hxCAU523jWUFRXMhM-p3w66P3PxYfY_tZm9pPeShU\",\n",
    "\"KwvOafjkobVhF8Iz63lMyNooS-hjaYgmHdZrqVVVHWw\",\n",
    "\"N5qb2q5dSfly3fnqWMsDPqVMWUd1yRbTBxbT34hvw_g\",\n",
    "\"SsQNhtNfemZJr_FzCYZlub29VeTTjymT3Qm6syC50W8\",\n",
    "\"br4m9W3KMhrPa51Vru5M1mWJYCfaVFvWiP3kB2gEQUA\",\n",
    "\"spdVYGb-b6cgokZBOuwGG4b3WbmgsxQqv7tudjdOfLc\",\n",
    "\"0zeR-GEwo5Od-poIZYTGrw-I80fC2C0b0q4F1Boa8oc\",\n",
    "\"UfHD68dH4O9ld-NS22Se6Bls98P9aPZhFq4hA_H7-74\",\n",
    "\"c6NrKs_BpAX4OdRFKAVSDeUaQTjSpNF8dCxjcLT6AfE\",\n",
    "\"WCpDyLiAACoL9zwSFQQa_HboP-HdWXSM601W1N02aeg\",\n",
    "\"faJrm5egL12NmLivrDsbqz44I47DhbJQUIg89L3TdIo\",\n",
    "\"cdXQHZ3zzsZv-n04kjimcml7e3BRKcR2H3wl8XPi6SU\",\n",
    "\"RBRLSkMgpwy6QF0jKsXs-jXEliqtAwwD_jd0DbHrEN8\",\n",
    "\"Dd-n66bTKOIKeQ0H6nRvIOMWatnJ_viPxKMNGpxsFa4\",\n",
    "\"q-Ao_RRfZNyfQPp2XsnYGbrOItF6SlXN0jObPIKVnk\",\n",
    "\"ufLjrWIrvjyLHzL1TZIEOZjKpgolDMBUPXUwoIblgs0\",\n",
    "\"tvjh1g0Xygp6DMsH8n5hX41jgbNh3VE61MsnWHYkfZw\",\n",
    "\"d_8_FhktHjeMh91gB9bBKGHgMJFC6auZqHThhlHL9Hw\",\n",
    "\"P1RAwTjkXvZUQLjHyi06Zm1YP1KeaZMJdjaNN1ZeOeE\",\n",
    "\"DQV3KQ2U9DLoro0jMQUoIr-PQMKBpKIEpOqXzF1A498\",\n",
    "\"6IcCytBh4PkKOC3YE2Smw4IGdAyTD7XTtsmgtxF-kdY\",\n",
    "\"PgxoArILmRDFQreVp3pC6bexYfcKIVwMMN4oKpa8VOo\",\n",
    "\"mwA5L1GvWFvnwIYL-U1kP0RDsZ61ElOKAH7wnbsTs4M\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hàm gọi api here để lấy thông tin vị trí từ địa chỉ chi tiết\n",
    "import requests\n",
    "\n",
    "def get_location(address, api_key ):\n",
    "    #api_key = \"mwA5L1GvWFvnwIYL-U1kP0RDsZ61ElOKAH7wnbsTs4M\"\n",
    "\n",
    "    url = f'https://geocode.search.hereapi.com/v1/geocode?q={address}&apiKey={api_key}'\n",
    "    \n",
    "    try:\n",
    "        # Gửi yêu cầu API\n",
    "        response = requests.get(url)\n",
    "        \n",
    "        # Nếu API key đã hết hạn, thử API key tiếp theo trong danh sách\n",
    "\n",
    "        if response.status_code == 429:\n",
    "            return {\"lat\": \"\", \"lng\": \"\", \"House_number\": \"\", \"Street\": \"\", \"Ward\": \"\", \"District\": \"\", \"City\": \"\", \"progress\": \"False\" }\n",
    "        else: \n",
    "            data = response.json()    \n",
    "            # Xử lý kết quả và trả về\n",
    "            if data == {'items': []}: \n",
    "                list_address = {\"lat\": \"\", \"lng\": \"\", \"House_number\": \"\", \"Street\": \"\", \"Ward\": \"\", \"District\": \"\", \"City\": \"\", \"progress\": \"True\" }\n",
    "            else: \n",
    "                result = data['items'][0]\n",
    "                position = result['position']\n",
    "                lat = position.get('lat', '')\n",
    "                long = position.get('lng', '')\n",
    "                address_info = result['address']\n",
    "                city = address_info.get('county', '')\n",
    "                district = address_info.get('city', '')\n",
    "                ward = address_info.get('district', '')\n",
    "                street = address_info.get('street', '')\n",
    "                houseNumber = address_info.get('houseNumber', '')\n",
    "                list_address = {\"lat\": lat, \"lng\": long, \"House_number\": houseNumber, \"Street\": street, \"Ward\": ward, \"District\": district, \"City\": city, \"progress\": \"True\" }\n",
    "    except:\n",
    "           # Nếu thử xảy ra lỗi trả ra tập rỗng và progess là False đánh dấu những case chưa xử lí\n",
    "        list_address = {\"lat\": \"\", \"lng\": \"\", \"House_number\": \"\", \"Street\": \"\", \"Ward\": \"\", \"District\": \"\", \"City\": \"\", \"progress\": \"False\" }\n",
    "           \n",
    "    return list_address\n",
    "\n",
    "# Hàm xử lí dữ liệu đầu vào là dataframe\n",
    "def out_put(df, api_key):\n",
    "    house_numbers = []\n",
    "    streets = []\n",
    "    wards = []\n",
    "    districts = []\n",
    "    cities = []\n",
    "    progresses = []\n",
    "    \n",
    "    for address in df[\"address\"]:\n",
    "        address_info = get_location(address,api_key)\n",
    "        house_numbers.append(address_info[\"House_number\"])\n",
    "        streets.append(address_info[\"Street\"])\n",
    "        wards.append(address_info[\"Ward\"])\n",
    "        districts.append(address_info[\"District\"])\n",
    "        cities.append(address_info[\"City\"])\n",
    "        progresses.append(address_info[\"progress\"])\n",
    "    \n",
    "    df[\"House_number\"] = house_numbers\n",
    "    df[\"Street\"] = streets\n",
    "    df[\"Ward\"] = wards\n",
    "    df[\"District\"] = districts\n",
    "    df[\"City\"] = cities\n",
    "    df[\"progress\"] = progresses\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the number of groups\n",
    "num_groups = len(df_test) // 1000\n",
    "\n",
    "# Calculate the number of remaining rows\n",
    "remaining_rows = len(df_test) % 1000\n",
    "\n",
    "# Create a list to store the new dataframes\n",
    "dfs = []\n",
    "\n",
    "# Split the dataframe into groups of two rows each\n",
    "for i in range(num_groups):\n",
    "    start_index = i * 1000\n",
    "    end_index = start_index + 1000\n",
    "    df_group = df_test[start_index:end_index].copy()\n",
    "    dfs.append(df_group)\n",
    "\n",
    "# Add the remaining rows to the last dataframe\n",
    "if remaining_rows > 0:\n",
    "    last_df = df_test[-remaining_rows:].copy()\n",
    "    dfs.append(last_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "# tiến hành chạy multi thread\n",
    "import threading\n",
    "import time\n",
    "\n",
    "final = []\n",
    "def out_put_with_time(df, api_key):\n",
    "    start_time = time.time()\n",
    "    out_put(df, api_key)\n",
    "    end_time = time.time()\n",
    "    execution_time = end_time - start_time\n",
    "    print(f\"Time: {execution_time/60} phút\")\n",
    "\n",
    "for df , api_key in zip(dfs, list_api_keys):\n",
    "    thread = threading.Thread(target=out_put_with_time, args=(df,api_key))\n",
    "    thread.start()\n",
    "    thread.join()\n",
    "    final.append(df)\n",
    "\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "No objects to concatenate",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[16], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m merged_df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconcat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfinal\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m#check case false\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# merged_df[merged_df[\"progress\"] == \"False\"]\u001b[39;00m\n\u001b[0;32m      5\u001b[0m merged_df\u001b[38;5;241m.\u001b[39mhead(\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\pandas\\core\\reshape\\concat.py:380\u001b[0m, in \u001b[0;36mconcat\u001b[1;34m(objs, axis, join, ignore_index, keys, levels, names, verify_integrity, sort, copy)\u001b[0m\n\u001b[0;32m    377\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m copy \u001b[38;5;129;01mand\u001b[39;00m using_copy_on_write():\n\u001b[0;32m    378\u001b[0m     copy \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m--> 380\u001b[0m op \u001b[38;5;241m=\u001b[39m \u001b[43m_Concatenator\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    381\u001b[0m \u001b[43m    \u001b[49m\u001b[43mobjs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    382\u001b[0m \u001b[43m    \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    383\u001b[0m \u001b[43m    \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    384\u001b[0m \u001b[43m    \u001b[49m\u001b[43mjoin\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    385\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkeys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    386\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlevels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlevels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    387\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnames\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnames\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    388\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverify_integrity\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverify_integrity\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    389\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    390\u001b[0m \u001b[43m    \u001b[49m\u001b[43msort\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msort\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    391\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    393\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m op\u001b[38;5;241m.\u001b[39mget_result()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\pandas\\core\\reshape\\concat.py:443\u001b[0m, in \u001b[0;36m_Concatenator.__init__\u001b[1;34m(self, objs, axis, join, keys, levels, names, ignore_index, verify_integrity, copy, sort)\u001b[0m\n\u001b[0;32m    440\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverify_integrity \u001b[38;5;241m=\u001b[39m verify_integrity\n\u001b[0;32m    441\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcopy \u001b[38;5;241m=\u001b[39m copy\n\u001b[1;32m--> 443\u001b[0m objs, keys \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_clean_keys_and_objs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobjs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeys\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    445\u001b[0m \u001b[38;5;66;03m# figure out what our result ndim is going to be\u001b[39;00m\n\u001b[0;32m    446\u001b[0m ndims \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_ndims(objs)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\pandas\\core\\reshape\\concat.py:505\u001b[0m, in \u001b[0;36m_Concatenator._clean_keys_and_objs\u001b[1;34m(self, objs, keys)\u001b[0m\n\u001b[0;32m    502\u001b[0m     objs_list \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(objs)\n\u001b[0;32m    504\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(objs_list) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m--> 505\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo objects to concatenate\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    507\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m keys \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    508\u001b[0m     objs_list \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(com\u001b[38;5;241m.\u001b[39mnot_none(\u001b[38;5;241m*\u001b[39mobjs_list))\n",
      "\u001b[1;31mValueError\u001b[0m: No objects to concatenate"
     ]
    }
   ],
   "source": [
    "merged_df = pd.concat(final, ignore_index=True)\n",
    "\n",
    "#check case false\n",
    "# merged_df[merged_df[\"progress\"] == \"False\"]\n",
    "merged_df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df = merged_df[merged_df[\"progress\"] != \"False\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_street_name(address):\n",
    "    if pd.isna(address):  # Kiểm tra xem giá trị có phải là NaN không\n",
    "        return None\n",
    "\n",
    "    # Sử dụng biểu thức chính quy để tìm tên đường\n",
    "    match = re.search(r'(?:Hẻm\\s*\\d+\\s*)|(?:Ngõ\\s*\\S+\\s*)|(?:Ngách\\s*\\S+\\s*)', address)\n",
    "    \n",
    "    if match:\n",
    "        return re.sub(r'(?:Hẻm\\s*\\d+\\s*)|(?:Ngõ\\s*\\S+\\s*)|(?:Ngách\\s*\\S+\\s*)', '', address)\n",
    "    else:\n",
    "        return address"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_location_prefix_preferred(address):\n",
    "    if pd.isna(address):  # Kiểm tra xem giá trị có phải là NaN không\n",
    "        return None\n",
    "\n",
    "    # Tìm kiếm từ khóa \"phường\", \"xã\", \"thị trấn\", \"thị xã\" và lấy phần trước đó\n",
    "    match_keyword = re.search(r'(.+?)(?i:phường|xã|thị trấn|thị xã)', address)\n",
    "    \n",
    "    if match_keyword:\n",
    "        result = match_keyword.group(1).strip()\n",
    "    else:\n",
    "        # Nếu không tìm thấy từ khóa, tìm kiếm từ đầu tiên cho đến dấu ','\n",
    "        match_first_part = re.search(r'([^,]+[a-zA-Z]+[^,]*)', address)\n",
    "        \n",
    "        if match_first_part:\n",
    "            result = match_first_part.group(1).strip()\n",
    "        else:\n",
    "            result = None\n",
    "    \n",
    "    # Xóa dấu ',' nếu có\n",
    "    if result is not None:\n",
    "        result = result.replace(',', '').replace('.','').replace('/','')\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merged_df['merged_df'] = merged_df['Street'].apply(extract_street_name)\n",
    "# df_fix_address['Location_Prefix_Preferred'] = df_fix_address['address'].apply(extract_location_prefix_preferred)\n",
    "# # df_fix_address.dropna()\n",
    "# # df_fix_address.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def final_street(row):\n",
    "#     if not pd.isna(row['Street_fix']):\n",
    "#         return row['Street_fix']\n",
    "#     else:\n",
    "#         return row['Location_Prefix_Preferred']\n",
    "# df_fix_address['Street_final'] = df_fix_address.apply(final_street,axis=1)\n",
    "# df_fix_address.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df['price_value_trieu'] = merged_df['price_per_m2_final']*merged_df['area_value']/1000000\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final_sosanhnha = merged_df[['City','District','Ward','Street','address','area_value','price_value_trieu','price_per_m2_final']]\n",
    "# df_final_sosanhnha.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final_sosanhnha.to_csv(r\"D:\\OneDrive\\KiotViet\\Python_for_work\\KFinance\\CSV_crawl_data\\final_bds_sosanhnha.csv\", index=False,mode = 'w', encoding=\"utf-8-sig\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "huyenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
